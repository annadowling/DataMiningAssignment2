{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PCA\n",
    "\n",
    "PCA is used for dimensionality reduction. This notebook is based on [In Depth: Principal Component Analysis](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html), with extensions to show what PCA means in practice."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some functions for later use. `drawVector` allows us to show the pricipal component axes on a plot. `abline` allows us to draw a line, given its slope and intercept, analogous to R's `abline` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# See https://stackoverflow.com/a/43811762\n",
    "def abline(slope, intercept):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We generate some 2-D random data points. The point cloud has an elongated shape by design. Therefore the first principal component is expected to align with the main axis of the data, and the second principal component would be orthogonal to the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "print(X[0:4,:])\n",
    "\n",
    "#X = np.random.randint(2, size=(100,30))\n",
    "#print(X[0:4,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have the 2-D point cloud, we derive its first and second principal components and display how much variance is explained by the first and second. As can be seen, most of the variance is associated with the first."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "explainedVariance = pca.explained_variance_\n",
    "v1=explainedVariance[0]\n",
    "v2=explainedVariance[1]\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "explainedVariance[0]": "0.7625315008826112",
     "explainedVariance[1]": "0.018477895513562572"
    }
   },
   "source": [
    "In other words, the first PC explains {{explainedVariance[0]}} with a variance of {{explainedVariance[1]}} left over for the second (and last) PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = pca.components_\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "e[0][0]": "-0.9444602872084233",
     "e[0][1]": "-0.32862557095603917",
     "e[1][0]": "-0.32862557095603917",
     "e[1][1]": "0.9444602872084233"
    }
   },
   "source": [
    "The PC components above are the eigenvectors that define the directions of the first and second PCs. After translating the data so its mean is at the origin, the first PC can be written in the form `({{e[0][0]}})x_1 + ({{e[0][1]}})x_2 = 0` and the second PC is perpendicular to it and can be written in the form `({{e[1][0]}})x_1 + ({{e[1][1]}})x_2 = 0`.\n",
    "\n",
    "We can now plot the data (in its original position) and overlay `PC_1` which runs along the main axis of the data and `PC_2` which is perpendicular to it. Note that the principal components have been scaled according to the amount of variance they explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.1)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('input')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot principal components\n",
    "X_pca = pca.transform(X)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0, 0], [0, 3])\n",
    "draw_vector([0, 0], [3, 0])\n",
    "plt.axis('equal')\n",
    "plt.xlabel(\"component 1 pc1\")\n",
    "plt.ylabel('component 2 pc2')\n",
    "plt.title('principal components')\n",
    "plt.xlim=(-5, 5)\n",
    "plt.ylim=(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we rerun the PCVA operation, but this time decide to keep only the leading principal components that explain at least 95% of the variance, we see that the second principal component can be ignored and we find a lower dimensional representation `X_trans` of the original data (1-D instead of 2-D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PCA(0.95) # keep 95% of variance\n",
    "X_trans = clf.fit_transform(X)\n",
    "print(X.shape)\n",
    "print(X_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the reduced dimension `X_trans` to the original `X` - see the following plot. As you can see, the original `X` points have been projected onto the PC_1 line. Because the points lie upon a line, each point maps to a value on the `PC_1` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = clf.inverse_transform(X_trans)\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.1)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\n",
    "ab1 = clf.components_[0]\n",
    "abline(slope=ab1[1]/ab1[0], intercept=0)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the 1-D nature of `X_trans` more obvious by rotating both the original `X` and the transformed `X_trans` so that their `PC_1` is horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the rotation parameters (Cosine and Sine) of the rotation angle\n",
    "C = -ab1[0]\n",
    "S = ab1[1]\n",
    "Q = np.vstack([[C, S], [-S, C]])\n",
    "# Xrot is the points rotated so they are aligned with the first principal component direction\n",
    "Xrot = np.matmul(X,Q)\n",
    "X_newrot = np.matmul(X_new,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xrot[:, 0], Xrot[:, 1], 'ob', alpha=0.1)\n",
    "plt.plot(X_newrot[:, 0], X_newrot[:, 1], 'ob', alpha=0.8)\n",
    "plt.axis('equal')\n",
    "abline(slope=0, intercept=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of analysis used PCA to reduce 2-D data to 1-D data, but of course it can be used to reduce a general high-dimensional data set to a lower-dimensional equivalent, in a way that keeps as much of the variance as possible. Also, because PCA derives principal components in order of decreasing variance, it is relatively easy to define criteria (such as \"keep the first 3 principal components\" or \"keep 80% of the variance\". Aloso, note that the set of principal components has the nice property that each principal component is orthogonal (perpendicular) to every other principal component, which means they are uncorrelated and this property makes operations like regression and classification more efficient.\n",
    "\n",
    "Lastly, if you require more intuition:\n",
    "\n",
    "* this [youtube video](https://www.youtube.com/watch?v=FgakZw6K1QQ) provides a very nice, graphical explanation of PCA, although (BAM!) you might find the presenter a bit annoying!\n",
    "* this [webpage](http://setosa.io/ev/principal-component-analysis/) also has some graphics. Perhaps the most interesting point is that the transformed and rotated data makes the underlying three blobs (clusters) in the data very obvious, so they would be easily found by any clustering algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
