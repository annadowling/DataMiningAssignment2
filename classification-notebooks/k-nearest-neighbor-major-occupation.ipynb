{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - k-nearest-neighbors - Major Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=8, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import feature subset with Major_Occupation Column and one hot encoded values\n",
    "\n",
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "\n",
    "originalDF = pd.read_csv('occupationFeatureSubset.csv')\n",
    "dfOHE = pd.read_csv('oheTransformedData.csv')\n",
    "dfOHE.fillna(0, inplace=True)\n",
    "\n",
    "X = dfOHE\n",
    "\n",
    "#separate target values\n",
    "y = originalDF['Major_Occupation'].values\n",
    "\n",
    "# create the model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "# fit the model\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With knn, you can determine membership probabilities for each of the 3 labels. As you can see, the predict() function just picks the most likely label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer']\n"
     ]
    }
   ],
   "source": [
    "# What kind of occupation has years on internet (1-3), web ordering (yes),Not_Purchasing_Security, age(35) \n",
    "# call the \"predict\" method:\n",
    "result = knn.predict([[1,0,0,0,0,1,0,0,0,0,0,0,1,35],])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.375, 0.25 , 0.   , 0.125, 0.25 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba([[1,0,0,0,0,1,0,0,0,0,0,0,1,35],]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code, we take each pair of predictors from the four available in the Iris data set, and use the k-nearest-neighbour algorithm with k=3,5,7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'w6support'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1c0f51d5945c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mw6support\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_2d_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create color maps for 3-class classification problem, as with iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'w6support'"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import itertools\n",
    "import re, string\n",
    "from w6support import plot_2d_class\n",
    "\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFDDDD', '#DDFFDD', '#DDDDFF'])\n",
    "cmap_bold = ListedColormap(['#FF2222', '#22FF22', '#8888FF'])\n",
    "\n",
    "#predNames = list(iris.data) # https://stackoverflow.com/a/19483025, except iris.data is an array, not a dataframe\n",
    "predNames = dfOHE.columns\n",
    "df=pd.DataFrame(dfOHE, columns=predNames)\n",
    "nTrain = df.shape[0]\n",
    "y = originalDF['Major_Occupation'].values\n",
    "pattern = re.compile('[\\W_]+', re.UNICODE) # https://stackoverflow.com/a/1277047\n",
    "for neighborCnt in range(3,8,2): # from 3 to a maximum of 8, in steps of 2, so 3,5,7\n",
    "  knn = neighbors.KNeighborsClassifier(n_neighbors=neighborCnt)\n",
    "  for twoCols in itertools.combinations(predNames, 2): # https://stackoverflow.com/a/374645\n",
    "    X = df[list(twoCols)]  # we only take two features at a time\n",
    "    colNames = X.columns\n",
    "    c1 = colNames[:1][0] # first of 2\n",
    "    c2 = colNames[-1:][0] # last of 2\n",
    "    c1 = pattern.sub(\"\",c1.title()) # Make titlecase, then remove non-alphanumeric characters\n",
    "    c2 = pattern.sub(\"\",c2.title())\n",
    "    knn.fit(X, y)\n",
    "    plotTitle = \"k = %i %s fit to the %s dataset\" % (neighborCnt, \"nearest-neighbours\", \"Iris\")\n",
    "    fileTitle = \"../../pic/k_%i_%s_%s_%s_%s.pdf\" % (neighborCnt, \"nearest-neighbours\", \"Iris\", c1, c2)\n",
    "    print(\"Plotting file %s\" % (fileTitle))\n",
    "    plot_2d_class(X, y, nTrain, knn, plotTitle, fileTitle, cmap_light, cmap_bold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "The k-nearest-neighbours classification \"model\" should be validated. Clearly, the parameter $k$ is critical to its performance. Generally, smaller values of $k$ fit the training set more accurately (less bias) but generalise less well to test data (more variance). The opposite applies to larger values of $k$.\n",
    "\n",
    "With $k$ set to its minimum value ($k = 1$), it fits the training set exactly and the confusion matrix is optimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = iris.data, iris.target\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X, y)\n",
    "y_pred1 = knn1.predict(X)\n",
    "print(np.all(y == y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *confusion matrix* highlights where classification differences arise, as these occur on the off-diagognal elements of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      1.000     1.000     1.000        50\n",
      "          1      1.000     1.000     1.000        50\n",
      "          2      1.000     1.000     1.000        50\n",
      "\n",
      "avg / total      1.000     1.000     1.000       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "print(accuracy_score(y, y_pred1))\n",
    "print(confusion_matrix(y, y_pred1))\n",
    "print(classification_report(y, y_pred1, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 50 training samples for each class are identified correctly, as expected when $k = 1$ (accuracy score is 1, off-diagonal terms are 0, the classification report (relative to the trsining set) is \"too good to be true\"...\n",
    "\n",
    "Note:\n",
    "\n",
    "1. The _Recall_ of the $i^{\\mbox{th}}$ predictor is $R_i \\equiv c_{ii} / \\sum_j c_{ij}$, which is the ratio of the $i^{\\mbox{th}}$ diagonal element to the sum of the elements of the confusion matrix $C = \\{c_{ij}\\}$ in that _column_.\n",
    "2. The _Precision_ of the $j^{\\mbox{th}}$ predictor is $P_j \\equiv c_{jj} / \\sum_i c_{ij}$, which is the ratio of the $j^{\\mbox{th}}$ diagonal element to the sum of the elements of the confusion matrix $C = \\{c_{ij}\\}$ in that _row_.\n",
    "3. $F_1$-score is defined as $F_1 = 2\\frac{R_i P_i}{R_i + P_i}$.\n",
    "\n",
    "To test how the model generalizes to the training set, we hold back some of the training data by splitting the training data into a _training set_ and a _testing set_. We hold back 20% and stratify based on the data labels $y$, so each of the row counts in the confusion matrix should be $0.2 * 50 = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "[[10  0  0]\n",
      " [ 0  8  2]\n",
      " [ 0  0 10]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      1.000     1.000     1.000        10\n",
      "          1      1.000     0.800     0.889        10\n",
      "          2      0.833     1.000     0.909        10\n",
      "\n",
      "avg / total      0.944     0.933     0.933        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "knn1.fit(Xtrain, ytrain)\n",
    "ypred1s = knn1.predict(Xtest)\n",
    "print(accuracy_score(ytest, ypred1s))\n",
    "print(confusion_matrix(ytest, ypred1s))\n",
    "print(classification_report(ytest, ypred1s, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the confusion (off-diagonal nonzero elements) between Iris species 2 and species 3. For comparison, we look at the confusion matrix when $k = 3$. Firstly, we try with all the training data (not holding any observations back for a test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      1.000     1.000     1.000        50\n",
      "          1      0.940     0.940     0.940        50\n",
      "          2      0.940     0.940     0.940        50\n",
      "\n",
      "avg / total      0.960     0.960     0.960       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn3.fit(X, y)\n",
    "y_pred3 = knn3.predict(X)\n",
    "print(accuracy_score(y, y_pred3))\n",
    "print(confusion_matrix(y, y_pred3))\n",
    "print(classification_report(y, y_pred3, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 6 observations (3 each of species 2 and 3) are not classified the same as the human experts. However, this might also indicate something interesting about those observations. They could be outliers (not classified correctly) but, at the very least, they are extreme observations.\n",
    "\n",
    "Now we try holding back 20% of the training set for use as test observations, leaving 80% of the training data to train the classifier. We then look at what happens to the confusion matrix. Note that sampling the data like this could result in *better* relative performance, depending on what happens to the 6 problematic observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "[[10  0  0]\n",
      " [ 0  8  2]\n",
      " [ 0  0 10]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      1.000     1.000     1.000        10\n",
      "          1      1.000     0.800     0.889        10\n",
      "          2      0.833     1.000     0.909        10\n",
      "\n",
      "avg / total      0.944     0.933     0.933        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn3.fit(Xtrain, ytrain)\n",
    "ypred3s = knn3.predict(Xtest)\n",
    "print(accuracy_score(ytest, ypred3s))\n",
    "print(confusion_matrix(ytest, ypred3s))\n",
    "print(classification_report(ytest, ypred3s, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
